{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --train_tsvs TRAIN_TSVS [TRAIN_TSVS ...]\n",
      "                             --test_tsvs TEST_TSVS [TEST_TSVS ...]\n",
      "                             [--model_dir MODEL_DIR] [--seed SEED]\n",
      "                             [--max_len MAX_LEN] [--batch_size BATCH_SIZE]\n",
      "                             [--epochs EPOCHS] [--lr LR] [--img_size IMG_SIZE]\n",
      "                             [--min_freq MIN_FREQ] [--num_workers NUM_WORKERS]\n",
      "                             [--save_every SAVE_EVERY]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --train_tsvs, --test_tsvs\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "# multimodal_damage_improved.py\n",
    "# HARD-CODED VERSION (Option C)\n",
    "# -------------------------------------------------------------------------\n",
    "# This version removes ALL argparse usage and works exactly like your original\n",
    "# script: just run it in a notebook or Python file, no CLI arguments required.\n",
    "# All config values and file paths are defined below. Nothing else.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "import os, re, random, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ======================================================================\n",
    "# ------------------------------ CONFIG --------------------------------\n",
    "# ======================================================================\n",
    "SEED = 42\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "LR = 2e-4\n",
    "IMG_SIZE = 128\n",
    "MIN_FREQ = 2\n",
    "NUM_WORKERS = 4\n",
    "MODEL_DIR = \"./checkpoints_multimodal_improved\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"/Volumes/Extreme SSD/DL_Proj/CrisisMMD_v2.0/crisismmd_datasplit_all/task_damage_text_img_dev.tsv\",\n",
    "    \"/Volumes/Extreme SSD/DL_Proj/CrisisMMD_v2.0/crisismmd_datasplit_all/task_damage_text_img_train.tsv\",\n",
    "]\n",
    "TEST_FILES = [\n",
    "    \"/Volumes/Extreme SSD/DL_Proj/CrisisMMD_v2.0/crisismmd_datasplit_all/task_damage_text_img_test.tsv\",\n",
    "]\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# ======================================================================\n",
    "# --------------------------- REPRO SETUP -------------------------------\n",
    "# ======================================================================\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# ======================================================================\n",
    "# ----------------------------- LOADING ---------------------------------\n",
    "# ======================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\\\s.,!?']\", \" \", text)\n",
    "    text = re.sub(r\"\\\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def load_tsv(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].astype(str).apply(clean_text)\n",
    "    return df\n",
    "\n",
    "train_df = pd.concat([load_tsv(p) for p in TRAIN_FILES], ignore_index=True)\n",
    "test_df = pd.concat([load_tsv(p) for p in TEST_FILES], ignore_index=True)\n",
    "\n",
    "label_le = LabelEncoder()\n",
    "train_df[\"label_id\"] = label_le.fit_transform(train_df[\"label\"])\n",
    "test_df[\"label_id\"] = label_le.transform(test_df[\"label\"])\n",
    "\n",
    "n_classes = len(label_le.classes_)\n",
    "print(\"Classes:\", label_le.classes_.tolist())\n",
    "\n",
    "# ======================================================================\n",
    "# ------------------------------- TOKENIZER -----------------------------\n",
    "# ======================================================================\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    return re.findall(r\"\\\\b[\\\\w']+\\\\b\", text.lower())\n",
    "\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(basic_tokenizer(t))\n",
    "    vocab = {\"<unk>\":0, \"<pad>\":1, \"<cls>\":2}\n",
    "    for w,f in counter.items():\n",
    "        if f >= min_freq:\n",
    "            vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train_df[\"tweet_text\"], MIN_FREQ)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, \"vocab.json\"), \"w\") as f:\n",
    "    json.dump(vocab, f, indent=2)\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = basic_tokenizer(text)[:MAX_LEN-1]\n",
    "    ids = [vocab[\"<cls>\"]] + [vocab.get(t, vocab[\"<unk>\"]) for t in tokens]\n",
    "    ids += [vocab[\"<pad>\"]] * (MAX_LEN - len(ids))\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# ======================================================================\n",
    "# ------------------------------- DATASET -------------------------------\n",
    "# ======================================================================\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "class CrisisDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = encode_text(row.tweet_text)\n",
    "        try:\n",
    "            img = Image.open(row.image).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0,0,0))\n",
    "        img = img_transform(img)\n",
    "        label = torch.tensor(row.label_id, dtype=torch.long)\n",
    "        return {\"text\":text, \"image\":img, \"label\":label}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"text\": torch.stack([b[\"text\"] for b in batch]),\n",
    "        \"image\": torch.stack([b[\"image\"] for b in batch]),\n",
    "        \"label\": torch.stack([b[\"label\"] for b in batch])\n",
    "    }\n",
    "\n",
    "train_dataset = CrisisDataset(train_df)\n",
    "test_dataset = CrisisDataset(test_df)\n",
    "\n",
    "counts = train_df[\"label_id\"].value_counts().sort_index()\n",
    "weights = 1.0 / torch.tensor(counts.values, dtype=torch.float)\n",
    "sample_weights = [weights[l] for l in train_df[\"label_id\"]]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# ======================================================================\n",
    "# -------------------------------- MODEL -------------------------------\n",
    "# ======================================================================\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d, heads, ff):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d, heads, dropout=0.1)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d, ff), nn.ReLU(), nn.Dropout(0.1), nn.Linear(ff, d)\n",
    "        )\n",
    "        self.n1 = nn.LayerNorm(d)\n",
    "        self.n2 = nn.LayerNorm(d)\n",
    "    def forward(self, x):\n",
    "        a,_ = self.attn(x, x, x)\n",
    "        x = self.n1(x + a)\n",
    "        f = self.ff(x)\n",
    "        x = self.n2(x + f)\n",
    "        return x\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d=128, heads=4, layers=3, ff=256):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d, padding_idx=1)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d))\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d, heads, ff) for _ in range(layers)])\n",
    "    def forward(self, x):\n",
    "        b, s = x.size()\n",
    "        h = self.emb(x) + self.pos[:, :s]\n",
    "        h = h.transpose(0,1)\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h)\n",
    "        return h[0]\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class DamageNetMM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.text = TextEncoder(vocab_size)\n",
    "        self.img = ImageEncoder()\n",
    "        self.t_proj = nn.Sequential(nn.Linear(128,128), nn.Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "BT: translator not available -> disabling back-translation\n",
      "Classes: ['little_or_no_damage', 'mild_damage', 'severe_damage']\n",
      "Minority class ids: [0, 1]\n",
      "Oversampled dataset size: 6039\n",
      "Vocab size: 3455\n",
      "Model params: 3059459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 188/188 [00:39<00:00,  4.73it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Train Loss: 0.4902 | Val Acc: 0.6049\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.25      0.21      0.23        71\n",
      "        mild_damage       0.36      0.10      0.15       126\n",
      "      severe_damage       0.67      0.88      0.76       332\n",
      "\n",
      "           accuracy                           0.60       529\n",
      "          macro avg       0.43      0.40      0.38       529\n",
      "       weighted avg       0.54      0.60      0.55       529\n",
      "\n",
      "Retrying 1856 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 0.5660\n",
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 188/188 [00:40<00:00,  4.60it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Train Loss: 0.4284 | Val Acc: 0.6030\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.22      0.18      0.20        71\n",
      "        mild_damage       0.45      0.16      0.24       126\n",
      "      severe_damage       0.67      0.86      0.75       332\n",
      "\n",
      "           accuracy                           0.60       529\n",
      "          macro avg       0.45      0.40      0.40       529\n",
      "       weighted avg       0.56      0.60      0.56       529\n",
      "\n",
      "Retrying 1368 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 0.6188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 188/188 [00:40<00:00,  4.66it/s, loss=0.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Train Loss: 0.3622 | Val Acc: 0.6144\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.25      0.17      0.20        71\n",
      "        mild_damage       0.42      0.20      0.27       126\n",
      "      severe_damage       0.68      0.87      0.76       332\n",
      "\n",
      "           accuracy                           0.61       529\n",
      "          macro avg       0.45      0.41      0.41       529\n",
      "       weighted avg       0.56      0.61      0.57       529\n",
      "\n",
      "Retrying 974 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 0.7510\n",
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 188/188 [00:40<00:00,  4.59it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | Train Loss: 0.2754 | Val Acc: 0.5936\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.30      0.23      0.26        71\n",
      "        mild_damage       0.35      0.19      0.25       126\n",
      "      severe_damage       0.67      0.83      0.74       332\n",
      "\n",
      "           accuracy                           0.59       529\n",
      "          macro avg       0.44      0.41      0.42       529\n",
      "       weighted avg       0.55      0.59      0.56       529\n",
      "\n",
      "Retrying 651 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 0.7747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 188/188 [00:39<00:00,  4.74it/s, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | Train Loss: 0.2143 | Val Acc: 0.6200\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.36      0.23      0.28        71\n",
      "        mild_damage       0.43      0.23      0.30       126\n",
      "      severe_damage       0.68      0.85      0.75       332\n",
      "\n",
      "           accuracy                           0.62       529\n",
      "          macro avg       0.49      0.44      0.44       529\n",
      "       weighted avg       0.58      0.62      0.58       529\n",
      "\n",
      "Retrying 443 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 0.9017\n",
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 188/188 [00:39<00:00,  4.74it/s, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | Train Loss: 0.1744 | Val Acc: 0.6049\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.29      0.24      0.26        71\n",
      "        mild_damage       0.39      0.15      0.22       126\n",
      "      severe_damage       0.67      0.86      0.75       332\n",
      "\n",
      "           accuracy                           0.60       529\n",
      "          macro avg       0.45      0.42      0.41       529\n",
      "       weighted avg       0.55      0.60      0.56       529\n",
      "\n",
      "Retrying 529 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 1.1515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 188/188 [00:39<00:00,  4.78it/s, loss=0.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | Train Loss: 0.1396 | Val Acc: 0.6333\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.38      0.15      0.22        71\n",
      "        mild_damage       0.44      0.27      0.33       126\n",
      "      severe_damage       0.69      0.87      0.77       332\n",
      "\n",
      "           accuracy                           0.63       529\n",
      "          macro avg       0.50      0.43      0.44       529\n",
      "       weighted avg       0.59      0.63      0.59       529\n",
      "\n",
      "Retrying 303 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 1.0291\n",
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 188/188 [00:39<00:00,  4.76it/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | Train Loss: 0.1244 | Val Acc: 0.6219\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.33      0.14      0.20        71\n",
      "        mild_damage       0.41      0.17      0.24       126\n",
      "      severe_damage       0.67      0.90      0.76       332\n",
      "\n",
      "           accuracy                           0.62       529\n",
      "          macro avg       0.47      0.40      0.40       529\n",
      "       weighted avg       0.56      0.62      0.56       529\n",
      "\n",
      "Retrying 369 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 1.0566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 188/188 [00:39<00:00,  4.79it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | Train Loss: 0.1022 | Val Acc: 0.6219\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.33      0.20      0.25        71\n",
      "        mild_damage       0.42      0.17      0.25       126\n",
      "      severe_damage       0.67      0.88      0.76       332\n",
      "\n",
      "           accuracy                           0.62       529\n",
      "          macro avg       0.48      0.42      0.42       529\n",
      "       weighted avg       0.57      0.62      0.57       529\n",
      "\n",
      "Retrying 306 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 0.9123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 188/188 [00:40<00:00,  4.59it/s, loss=0.0943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Train Loss: 0.0943 | Val Acc: 0.6276\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.34      0.17      0.23        71\n",
      "        mild_damage       0.44      0.21      0.29       126\n",
      "      severe_damage       0.68      0.88      0.77       332\n",
      "\n",
      "           accuracy                           0.63       529\n",
      "          macro avg       0.49      0.42      0.43       529\n",
      "       weighted avg       0.58      0.63      0.58       529\n",
      "\n",
      "Retrying 255 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 0.8625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 188/188 [00:39<00:00,  4.71it/s, loss=0.0819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | Train Loss: 0.0819 | Val Acc: 0.6144\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.29      0.21      0.25        71\n",
      "        mild_damage       0.42      0.17      0.24       126\n",
      "      severe_damage       0.68      0.87      0.76       332\n",
      "\n",
      "           accuracy                           0.61       529\n",
      "          macro avg       0.46      0.42      0.42       529\n",
      "       weighted avg       0.56      0.61      0.57       529\n",
      "\n",
      "Retrying 262 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 0.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 188/188 [00:39<00:00,  4.82it/s, loss=0.0763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | Train Loss: 0.0763 | Val Acc: 0.6163\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.30      0.18      0.23        71\n",
      "        mild_damage       0.41      0.19      0.26       126\n",
      "      severe_damage       0.68      0.87      0.76       332\n",
      "\n",
      "           accuracy                           0.62       529\n",
      "          macro avg       0.46      0.41      0.42       529\n",
      "       weighted avg       0.56      0.62      0.57       529\n",
      "\n",
      "Retrying 218 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 1.0528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 188/188 [00:39<00:00,  4.74it/s, loss=0.0642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | Train Loss: 0.0642 | Val Acc: 0.6181\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage       0.29      0.21      0.25        71\n",
      "        mild_damage       0.46      0.19      0.27       126\n",
      "      severe_damage       0.68      0.87      0.76       332\n",
      "\n",
      "           accuracy                           0.62       529\n",
      "          macro avg       0.48      0.42      0.43       529\n",
      "       weighted avg       0.57      0.62      0.57       529\n",
      "\n",
      "Retrying 199 minority hard samples for 1 pass(es)\n",
      "  Hard pass 1 loss: 0.9218\n",
      "Early stopping.\n",
      "Training finished. Best val acc: 0.6332703213610587\n"
     ]
    }
   ],
   "source": [
    "# damagenet_text_final_improved_minority.py\n",
    "\"\"\"\n",
    "Final patched script focused on improving minority-class performance.\n",
    "Key changes:\n",
    " - Hard-example retries only for MINORITY misclassified samples (1 pass)\n",
    " - Minority token dropout increased to 0.20\n",
    " - Soft targets applied only to minority samples\n",
    " - Stronger logit adjustment scale\n",
    " - On-the-fly cheap augmentations for minority samples (random deletion / swap)\n",
    " - BT augmentation still available, augmented rows are tagged and excluded from hard mining\n",
    " - Safe BalancedBatchSampler (terminates)\n",
    "\"\"\"\n",
    "\n",
    "import os, re, json, random, time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "SEED = 42\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 30            # try to keep divisible by n_classes=3\n",
    "EPOCHS = 20\n",
    "LR = 2e-4\n",
    "\n",
    "EMB_DIM = 256\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 4\n",
    "FF_DIM = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "MODEL_DIR = \"./damagenet_text_minority_ckpt\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"/Volumes/Extreme SSD/DL_Proj/CrisisMMD_v2.0/crisismmd_datasplit_all/task_damage_text_img_train.tsv\",\n",
    "    \"/Volumes/Extreme SSD/DL_Proj/CrisisMMD_v2.0/crisismmd_datasplit_all/task_damage_text_img_dev.tsv\",\n",
    "]\n",
    "TEST_FILES = [\n",
    "    \"/Volumes/Extreme SSD/DL_Proj/CrisisMMD_v2.0/crisismmd_datasplit_all/task_damage_text_img_test.tsv\",\n",
    "]\n",
    "\n",
    "# Back-translation remains available\n",
    "ENABLE_BACKTRANSLATION = True\n",
    "BT_LANGS = [\"fr\", \"de\", \"es\", \"it\"]\n",
    "BT_CYCLES = 2\n",
    "\n",
    "# Minority-focused hyperparams (tuned for better minority recall)\n",
    "MINORITY_TOKEN_DROPOUT = 0.20   # increased\n",
    "USE_SOFT_LABELS = True\n",
    "SOFT_CONF = 0.88                # slightly higher confidence for minority soft targets\n",
    "\n",
    "# Hard-example mining\n",
    "HARD_RETRY_PASSES = 1           # reduced to 1\n",
    "HARD_SAMPLE_LIMIT = 2000\n",
    "\n",
    "# Cheap augmentations for minority samples per epoch\n",
    "MINORITY_AUG_USES_PER_EPOCH = 1  # how many augmented passes of each minority sample inserted per epoch\n",
    "MINORITY_AUG_PROB = 0.6          # probability to apply augmentation in dataset __getitem__ for minority\n",
    "\n",
    "# Data loader\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# -------------------- REPRO --------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "# -------------------- optional translator --------------------\n",
    "TRANSLATOR = None\n",
    "TRANSLATOR_NAME = None\n",
    "if ENABLE_BACKTRANSLATION:\n",
    "    try:\n",
    "        from googletrans import Translator as GoogleTranslator\n",
    "        TRANSLATOR = GoogleTranslator()\n",
    "        TRANSLATOR_NAME = \"googletrans\"\n",
    "        print(\"BT: using googletrans\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            from deep_translator import GoogleTranslator as DeepTranslator\n",
    "            TRANSLATOR = DeepTranslator(source=\"auto\", target=\"en\")\n",
    "            TRANSLATOR_NAME = \"deep_translator\"\n",
    "            print(\"BT: using deep_translator\")\n",
    "        except Exception:\n",
    "            TRANSLATOR = None\n",
    "            TRANSLATOR_NAME = None\n",
    "            ENABLE_BACKTRANSLATION = False\n",
    "            print(\"BT: translator not available -> disabling back-translation\")\n",
    "\n",
    "# -------------------- TEXT UTIL --------------------\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s.,!?']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    return re.findall(r\"\\b[\\w']+\\b\", text.lower())\n",
    "\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(basic_tokenizer(t))\n",
    "    vocab = {\"<pad>\":0, \"<unk>\":1, \"<cls>\":2}\n",
    "    for w,c in counter.items():\n",
    "        if c >= min_freq:\n",
    "            vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode_text_ids_from_tokens(tokens, vocab):\n",
    "    ids = [vocab.get(t, vocab[\"<unk>\"]) for t in tokens[:MAX_LEN-1]]\n",
    "    ids = [vocab[\"<cls>\"]] + ids\n",
    "    if len(ids) < MAX_LEN:\n",
    "        ids += [vocab[\"<pad>\"]] * (MAX_LEN - len(ids))\n",
    "    return ids[:MAX_LEN]\n",
    "\n",
    "def encode_text(text, vocab):\n",
    "    toks = basic_tokenizer(text)\n",
    "    return torch.tensor(encode_text_ids_from_tokens(toks, vocab), dtype=torch.long)\n",
    "\n",
    "# -------------------- LOAD DATA --------------------\n",
    "def load_tsv(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].astype(str).apply(clean_text)\n",
    "    return df\n",
    "\n",
    "train_df = pd.concat([load_tsv(p) for p in TRAIN_FILES], ignore_index=True)\n",
    "test_df  = pd.concat([load_tsv(p) for p in TEST_FILES], ignore_index=True)\n",
    "\n",
    "label_le = LabelEncoder()\n",
    "train_df[\"label_id\"] = label_le.fit_transform(train_df[\"label\"])\n",
    "test_df[\"label_id\"]  = label_le.transform(test_df[\"label\"])\n",
    "n_classes = len(label_le.classes_)\n",
    "print(\"Classes:\", label_le.classes_.tolist())\n",
    "\n",
    "# -------------------- targeted back-translation helpers --------------------\n",
    "def should_backtranslate(text):\n",
    "    toks = basic_tokenizer(text)\n",
    "    if len(toks) <= 7:\n",
    "        return True\n",
    "    if len(set(toks)) <= max(2, len(toks)//2):\n",
    "        return True\n",
    "    for tok in [\"collapsed\",\"destroyed\",\"flooded\",\"burned\",\"fire\",\"injured\",\"killed\",\"trapped\"]:\n",
    "        if tok in text:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def back_translate_googletrans(text, lang_chain):\n",
    "    try:\n",
    "        cur = text\n",
    "        for lang in lang_chain:\n",
    "            cur = TRANSLATOR.translate(cur, dest=lang).text\n",
    "        cur = TRANSLATOR.translate(cur, dest=\"en\").text\n",
    "        return clean_text(cur)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def back_translate_deeptrans(text, lang_chain):\n",
    "    try:\n",
    "        from deep_translator import GoogleTranslator as DeepTranslator\n",
    "        cur = text\n",
    "        for lang in lang_chain:\n",
    "            cur = DeepTranslator(source='auto', target=lang).translate(cur)\n",
    "        cur = DeepTranslator(source='auto', target='en').translate(cur)\n",
    "        return clean_text(cur)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def back_translate(text, lang_chain):\n",
    "    if TRANSLATOR is None:\n",
    "        return text\n",
    "    if TRANSLATOR_NAME == \"googletrans\":\n",
    "        return back_translate_googletrans(text, lang_chain)\n",
    "    elif TRANSLATOR_NAME == \"deep_translator\":\n",
    "        return back_translate_deeptrans(text, lang_chain)\n",
    "    return text\n",
    "\n",
    "# -------------------- BT augmentation (minority-targeted) --------------------\n",
    "class_counts = train_df[\"label_id\"].value_counts().sort_index()\n",
    "mean_count = class_counts.mean()\n",
    "minority_classes = class_counts[class_counts < mean_count].index.tolist()\n",
    "print(\"Minority class ids:\", minority_classes)\n",
    "\n",
    "# we'll tag rows that were BT-augmented so they can be excluded from hard mining\n",
    "train_df[\"_aug_bt\"] = False\n",
    "\n",
    "if ENABLE_BACKTRANSLATION and TRANSLATOR is not None:\n",
    "    aug_rows = []\n",
    "    subset = train_df[train_df[\"label_id\"].isin(minority_classes)].reset_index(drop=True)\n",
    "    print(\"BT: selecting minority samples for augmentation...\")\n",
    "    for _, row in tqdm(subset.iterrows(), total=len(subset), desc=\"BT select\"):\n",
    "        txt = row[\"tweet_text\"]\n",
    "        if not should_backtranslate(txt):\n",
    "            continue\n",
    "        for c in range(BT_CYCLES):\n",
    "            lang_chain = BT_LANGS[:]\n",
    "            random.shuffle(lang_chain)\n",
    "            aug_text = back_translate(txt, lang_chain)\n",
    "            new_row = row.copy()\n",
    "            new_row[\"tweet_text\"] = aug_text\n",
    "            new_row[\"_aug_bt\"] = True\n",
    "            aug_rows.append(new_row)\n",
    "    if aug_rows:\n",
    "        aug_df = pd.DataFrame(aug_rows)\n",
    "        train_df = pd.concat([train_df, aug_df], ignore_index=True).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "        print(f\"BT: added {len(aug_df)} augmented minority samples.\")\n",
    "\n",
    "# -------------------- physical oversampling --------------------\n",
    "def oversample_to_max(df):\n",
    "    counts = df[\"label_id\"].value_counts()\n",
    "    max_c = counts.max()\n",
    "    parts = []\n",
    "    for cls, cnt in counts.items():\n",
    "        subset = df[df[\"label_id\"] == cls]\n",
    "        times = int(np.ceil(max_c / max(1, cnt)))\n",
    "        parts.extend([subset] * times)\n",
    "    out = pd.concat(parts, ignore_index=True).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "    print(\"Oversampled dataset size:\", len(out))\n",
    "    return out\n",
    "\n",
    "train_df = oversample_to_max(train_df)\n",
    "\n",
    "# -------------------- cheap on-the-fly augmentations (minority) --------------------\n",
    "def random_deletion(tokens, p=0.2):\n",
    "    if len(tokens) <= 3: \n",
    "        return tokens\n",
    "    keep = [t for t in tokens if random.random() > p]\n",
    "    if len(keep) == 0:\n",
    "        return tokens[:max(1, len(tokens)//2)]\n",
    "    return keep\n",
    "\n",
    "def random_swap(tokens, n_swaps=1):\n",
    "    toks = tokens[:]\n",
    "    for _ in range(n_swaps):\n",
    "        if len(toks) < 2: break\n",
    "        i,j = random.sample(range(len(toks)), 2)\n",
    "        toks[i], toks[j] = toks[j], toks[i]\n",
    "    return toks\n",
    "\n",
    "# -------------------- vocab & dataset --------------------\n",
    "vocab = build_vocab(train_df[\"tweet_text\"].tolist(), min_freq=2)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "with open(os.path.join(MODEL_DIR, \"vocab.json\"), \"w\") as f:\n",
    "    json.dump(vocab, f, indent=2)\n",
    "\n",
    "minority_set = set(minority_classes)\n",
    "\n",
    "class BalancedTextDataset(Dataset):\n",
    "    def __init__(self, df, vocab, minority_set=None, token_dropout=0.0, aug_prob=0.0):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        self.minority_set = minority_set or set()\n",
    "        self.token_dropout = token_dropout\n",
    "        self.aug_prob = aug_prob\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"tweet_text\"]\n",
    "        label = int(row[\"label_id\"])\n",
    "        toks = basic_tokenizer(text)\n",
    "        # Minority cheap augmentation on-the-fly (random deletion / swap)\n",
    "        if label in self.minority_set and random.random() < self.aug_prob:\n",
    "            if random.random() < 0.5:\n",
    "                toks = random_deletion(toks, p=0.2)\n",
    "            else:\n",
    "                toks = random_swap(toks, n_swaps=1)\n",
    "        ids = encode_text_ids_from_tokens(toks, self.vocab)\n",
    "        ids = torch.tensor(ids, dtype=torch.long)\n",
    "        # minority token dropout\n",
    "        if self.token_dropout > 0 and label in self.minority_set:\n",
    "            prob = self.token_dropout\n",
    "            keep_mask = torch.rand(ids.size(0)) > prob\n",
    "            keep_mask[0] = True  # keep CLS\n",
    "            kept = ids[keep_mask].tolist()\n",
    "            kept = kept[:MAX_LEN]\n",
    "            if len(kept) < MAX_LEN:\n",
    "                kept += [self.vocab[\"<pad>\"]] * (MAX_LEN - len(kept))\n",
    "            ids = torch.tensor(kept, dtype=torch.long)\n",
    "        return {\"input_ids\": ids, \"label\": torch.tensor(label, dtype=torch.long), \"_aug_bt\": row.get(\"_aug_bt\", False)}\n",
    "\n",
    "train_dataset = BalancedTextDataset(train_df, vocab, minority_set=minority_set, token_dropout=MINORITY_TOKEN_DROPOUT, aug_prob=MINORITY_AUG_PROB)\n",
    "test_dataset  = BalancedTextDataset(test_df, vocab, minority_set=minority_set, token_dropout=0.0, aug_prob=0.0)\n",
    "\n",
    "# -------------------- SAFE BalancedBatchSampler --------------------\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, labels, batch_size):\n",
    "        self.labels = np.array(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.classes = np.unique(self.labels)\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.samples_per_class = max(1, batch_size // self.num_classes)\n",
    "        self.idx_by_class = {c: np.where(self.labels == c)[0].tolist() for c in self.classes}\n",
    "        for c in self.classes:\n",
    "            if len(self.idx_by_class[c]) == 0:\n",
    "                raise ValueError(f\"No samples found for class {c}.\")\n",
    "        smallest = min(len(v) for v in self.idx_by_class.values())\n",
    "        self.num_batches = max(1, smallest // self.samples_per_class)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __iter__(self):\n",
    "        pools = {c: np.random.permutation(v).tolist() for c, v in self.idx_by_class.items()}\n",
    "        ptrs = {c: 0 for c in self.classes}\n",
    "        for _ in range(self.num_batches):\n",
    "            batch = []\n",
    "            for c in self.classes:\n",
    "                start = ptrs[c]; end = start + self.samples_per_class\n",
    "                if end > len(pools[c]):\n",
    "                    pools[c] = np.random.permutation(pools[c]).tolist()\n",
    "                    ptrs[c] = 0\n",
    "                    start = 0; end = self.samples_per_class\n",
    "                batch.extend(pools[c][start:end])\n",
    "                ptrs[c] += self.samples_per_class\n",
    "            if len(batch) < self.batch_size:\n",
    "                all_idx = np.arange(len(self.labels))\n",
    "                extra = self.batch_size - len(batch)\n",
    "                batch.extend(np.random.choice(all_idx, extra, replace=False).tolist())\n",
    "            random.shuffle(batch)\n",
    "            yield batch\n",
    "\n",
    "train_sampler = BalancedBatchSampler(train_df[\"label_id\"].values, batch_size=BATCH_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# -------------------- model --------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True, dropout=dropout)\n",
    "        self.ff = nn.Sequential(nn.Linear(dim, ff_dim), nn.ReLU(), nn.Dropout(dropout), nn.Linear(ff_dim, dim))\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attn(x, x, x, key_padding_mask=mask)[0]\n",
    "        x = self.norm1(x + attn_out)\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=EMB_DIM, n_heads=N_HEADS, n_layers=N_LAYERS, ff_dim=FF_DIM):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, emb_dim))\n",
    "        self.layers = nn.ModuleList([TransformerBlock(emb_dim, n_heads, ff_dim, dropout=DROPOUT) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "    def forward(self, ids):\n",
    "        mask = ids.eq(0)\n",
    "        x = self.emb(ids) + self.pos[:, :ids.size(1)]\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask)\n",
    "        return self.norm(x[:, 0])\n",
    "\n",
    "class DamageNetTextFromScratch(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, emb_dim=EMB_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = TextEncoder(vocab_size)\n",
    "        self.classifier = nn.Sequential(nn.Linear(emb_dim, emb_dim//2), nn.ReLU(), nn.Dropout(0.3), nn.Linear(emb_dim//2, n_classes))\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    def forward(self, ids, logit_adjust=None):\n",
    "        z = self.encoder(ids)\n",
    "        logits = self.classifier(z)\n",
    "        if logit_adjust is not None:\n",
    "            logits = logits + logit_adjust\n",
    "        return logits\n",
    "\n",
    "# -------------------- Class-balanced loss --------------------\n",
    "class CBLoss(nn.Module):\n",
    "    def __init__(self, samples_per_class, beta=0.9999, gamma=2.0, device=DEVICE):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        effective_num = 1.0 - np.power(beta, samples_per_class)\n",
    "        weights = (1.0 - beta) / (effective_num + 1e-12)\n",
    "        weights = weights / np.sum(weights) * len(samples_per_class)\n",
    "        self.weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "    def forward(self, logits, targets):\n",
    "        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = ((1 - pt) ** self.gamma) * ce\n",
    "        w = self.weights[targets]\n",
    "        return (w * focal).mean()\n",
    "\n",
    "# -------------------- utilities --------------------\n",
    "def build_soft_targets(y_batch, n_classes, soft_conf=SOFT_CONF):\n",
    "    bs = y_batch.size(0)\n",
    "    soft = torch.full((bs, n_classes), (1.0 - soft_conf) / (n_classes - 1), device=y_batch.device)\n",
    "    for i, lab in enumerate(y_batch):\n",
    "        soft[i, lab] = soft_conf\n",
    "    return soft\n",
    "\n",
    "def collect_hard_examples_minority(model, dataset, minority_set, limit=HARD_SAMPLE_LIMIT):\n",
    "    \"\"\"\n",
    "    Collect misclassified training samples that are MINORITY and NOT BT-augmented.\n",
    "    Returns (xh, yh) or (None, None).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    xs, ys = [], []\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            x = b[\"input_ids\"].to(DEVICE)\n",
    "            y = b[\"label\"].to(DEVICE)\n",
    "            aug_bt_flags = b.get(\"_aug_bt\", None)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(1)\n",
    "            mask = preds != y\n",
    "            # filter only minority and non-BT augmented\n",
    "            mask_indices = []\n",
    "            if aug_bt_flags is None:\n",
    "                aug_mask = [False] * len(mask)\n",
    "            else:\n",
    "                aug_mask = aug_bt_flags\n",
    "            for i_val, m in enumerate(mask.cpu().numpy()):\n",
    "                if not m:\n",
    "                    continue\n",
    "                yi = int(y.cpu()[i_val].item())\n",
    "                if yi in minority_set and not bool(aug_mask[i_val]):\n",
    "                    mask_indices.append(i_val)\n",
    "            if len(mask_indices) > 0:\n",
    "                xs.append(x[mask_indices].cpu())\n",
    "                ys.append(y[mask_indices].cpu())\n",
    "    if not xs:\n",
    "        return None, None\n",
    "    xh = torch.cat(xs)[:limit]\n",
    "    yh = torch.cat(ys)[:limit]\n",
    "    return xh, yh\n",
    "\n",
    "def evaluate(model, loader, logit_adjust=None):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            x = b[\"input_ids\"].to(DEVICE)\n",
    "            y = b[\"label\"].to(DEVICE)\n",
    "            logits = model(x, logit_adjust)\n",
    "            preds.extend(logits.argmax(1).cpu().tolist())\n",
    "            trues.extend(y.cpu().tolist())\n",
    "    acc = accuracy_score(trues, preds) if len(trues) > 0 else 0.0\n",
    "    return acc, trues, preds\n",
    "\n",
    "# -------------------- training loop --------------------\n",
    "def train_loop():\n",
    "    model = DamageNetTextFromScratch(vocab_size=vocab_size, n_classes=n_classes).to(DEVICE)\n",
    "    print(\"Model params:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "    samples = train_df[\"label_id\"].value_counts().sort_index().values.astype(np.int64)\n",
    "    criterion_cb = CBLoss(samples_per_class=samples, beta=0.9999, gamma=2.0, device=DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "    # initial logit adjust: stronger boost to minority\n",
    "    priors = samples / samples.sum()\n",
    "    logit_adjust = torch.log(1.0 / (torch.tensor(priors, dtype=torch.float32) + 1e-12)).to(DEVICE)\n",
    "    logit_adjust = (logit_adjust - logit_adjust.mean()) * 1.0  # stronger scaling than before\n",
    "\n",
    "    best = 0.0\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        it = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        for batch in pbar:\n",
    "            it += 1\n",
    "            x = batch[\"input_ids\"].to(DEVICE)\n",
    "            y = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(x, logit_adjust)\n",
    "                    # apply soft targets only to minority items in batch\n",
    "                    if USE_SOFT_LABELS:\n",
    "                        mask_min = torch.tensor([int(int(li) in minority_set) for li in y.cpu().tolist()], device=DEVICE).bool()\n",
    "                        if mask_min.any():\n",
    "                            # create soft for minority rows and CE for rest\n",
    "                            y_min_idx = mask_min.nonzero(as_tuple=False).squeeze(1)\n",
    "                            y_min = y[y_min_idx]\n",
    "                            y_soft = build_soft_targets(y_min, n_classes)\n",
    "                            logits_min = logits[y_min_idx]\n",
    "                            loss_min = F.kl_div(F.log_softmax(logits_min, dim=1), y_soft, reduction=\"batchmean\")\n",
    "                            if (~mask_min).any():\n",
    "                                y_maj = y[~mask_min]\n",
    "                                logits_maj = logits[~mask_min]\n",
    "                                loss_maj = criterion_cb(logits_maj, y_maj)\n",
    "                                loss = 0.6 * loss_min + 0.4 * loss_maj\n",
    "                            else:\n",
    "                                loss = loss_min\n",
    "                        else:\n",
    "                            loss = criterion_cb(logits, y)\n",
    "                    else:\n",
    "                        loss = criterion_cb(logits, y)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                logits = model(x, logit_adjust)\n",
    "                if USE_SOFT_LABELS:\n",
    "                    mask_min = torch.tensor([int(int(li) in minority_set) for li in y.cpu().tolist()], device=DEVICE).bool()\n",
    "                    if mask_min.any():\n",
    "                        y_min_idx = mask_min.nonzero(as_tuple=False).squeeze(1)\n",
    "                        y_min = y[y_min_idx]\n",
    "                        y_soft = build_soft_targets(y_min, n_classes)\n",
    "                        logits_min = logits[y_min_idx]\n",
    "                        loss_min = F.kl_div(F.log_softmax(logits_min, dim=1), y_soft, reduction=\"batchmean\")\n",
    "                        if (~mask_min).any():\n",
    "                            y_maj = y[~mask_min]\n",
    "                            logits_maj = logits[~mask_min]\n",
    "                            loss_maj = criterion_cb(logits_maj, y_maj)\n",
    "                            loss = 0.6 * loss_min + 0.4 * loss_maj\n",
    "                        else:\n",
    "                            loss = loss_min\n",
    "                    else:\n",
    "                        loss = criterion_cb(logits, y)\n",
    "                else:\n",
    "                    loss = criterion_cb(logits, y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "            pbar.set_postfix(loss=total_loss/it)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        val_acc, val_trues, val_preds = evaluate(model, test_loader, logit_adjust)\n",
    "        print(f\"\\nEpoch {epoch} | Train Loss: {total_loss/it:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(classification_report(val_trues, val_preds, target_names=label_le.classes_, zero_division=0))\n",
    "\n",
    "        # small logit_adjust update based on per-class recall (mild)\n",
    "        report = classification_report(val_trues, val_preds, output_dict=True, zero_division=0)\n",
    "        recs = []\n",
    "        for i, cls in enumerate(label_le.classes_):\n",
    "            rec = report.get(cls, {}).get(\"recall\", 0.0)\n",
    "            recs.append(rec)\n",
    "        recs = np.array(recs)\n",
    "        adjust = (1.0 - recs)\n",
    "        adjust_t = torch.tensor(adjust, dtype=torch.float32, device=DEVICE)\n",
    "        logit_adjust = logit_adjust + (adjust_t - adjust_t.mean()) * 0.25  # slightly stronger update\n",
    "        logit_adjust = torch.clamp(logit_adjust, -4.0, 4.0)\n",
    "\n",
    "        # checkpoint\n",
    "        ckpt = {\"epoch\": epoch, \"model_state\": model.state_dict(), \"optim_state\": optimizer.state_dict(), \"logit_adjust\": logit_adjust.cpu().numpy()}\n",
    "        torch.save(ckpt, os.path.join(MODEL_DIR, f\"epoch_{epoch}.pth\"))\n",
    "\n",
    "        # hard example mining - only minority misclassified AND not BT-augmented\n",
    "        hard_x, hard_y = collect_hard_examples_minority(model, train_dataset, minority_set, limit=HARD_SAMPLE_LIMIT)\n",
    "        if hard_x is not None and HARD_RETRY_PASSES > 0:\n",
    "            hard_x = hard_x.to(DEVICE)\n",
    "            hard_y = hard_y.to(DEVICE)\n",
    "            print(f\"Retrying {len(hard_x)} minority hard samples for {HARD_RETRY_PASSES} pass(es)\")\n",
    "            for r in range(HARD_RETRY_PASSES):\n",
    "                model.train()\n",
    "                CH = 256\n",
    "                idx = 0\n",
    "                total_hloss = 0.0\n",
    "                it_h = 0\n",
    "                while idx < len(hard_x):\n",
    "                    xb = hard_x[idx: idx+CH]\n",
    "                    yb = hard_y[idx: idx+CH]\n",
    "                    optimizer.zero_grad()\n",
    "                    if scaler:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            logits = model(xb, logit_adjust)\n",
    "                            # for hard minority, use soft targets (makes them learn distribution)\n",
    "                            y_soft = build_soft_targets(yb, n_classes, soft_conf=SOFT_CONF)\n",
    "                            hloss = F.kl_div(F.log_softmax(logits, dim=1), y_soft, reduction=\"batchmean\")\n",
    "                        scaler.scale(hloss).backward()\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        scaler.step(optimizer); scaler.update()\n",
    "                    else:\n",
    "                        logits = model(xb, logit_adjust)\n",
    "                        y_soft = build_soft_targets(yb, n_classes, soft_conf=SOFT_CONF)\n",
    "                        hloss = F.kl_div(F.log_softmax(logits, dim=1), y_soft, reduction=\"batchmean\")\n",
    "                        hloss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        optimizer.step()\n",
    "                    total_hloss += float(hloss.item())\n",
    "                    it_h += 1\n",
    "                    idx += CH\n",
    "                print(f\"  Hard pass {r+1} loss: {total_hloss/it_h:.4f}\")\n",
    "\n",
    "        # best model\n",
    "        if val_acc > best:\n",
    "            best = val_acc\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"best_minority.pt\"))\n",
    "            print(\"Saved new best model.\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 6:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training finished. Best val acc:\", best)\n",
    "\n",
    "# -------------------- ENTRY --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: mps\n",
      "Loading vocab: /Volumes/Extreme SSD/DL_Proj/damagenet_text_minority_ckpt/vocab.json\n",
      "Vocab size: 3455\n",
      "Loading test file: /Volumes/Extreme SSD/DL_Proj/CrisisMMD_v2.0/crisismmd_datasplit_all/task_damage_text_img_test.tsv\n",
      "CLASSES: ['little_or_no_damage', 'mild_damage', 'severe_damage']\n",
      "Loading checkpoint: /Volumes/Extreme SSD/DL_Proj/damagenet_text_minority_ckpt/best_minority.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/pzq5xgxj58bb6nqgtmckp_x00000gn/T/ipykernel_94663/2444142208.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint format: raw state_dict dict\n",
      "No logit adjustment found.\n",
      "\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 20.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= TEST RESULTS =================\n",
      "Accuracy: 0.6276\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "little_or_no_damage     0.3714    0.1831    0.2453        71\n",
      "        mild_damage     0.4430    0.2778    0.3415       126\n",
      "      severe_damage     0.6843    0.8554    0.7604       332\n",
      "\n",
      "           accuracy                         0.6276       529\n",
      "          macro avg     0.4996    0.4388    0.4490       529\n",
      "       weighted avg     0.5849    0.6276    0.5915       529\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 13   9  49]\n",
      " [  9  35  82]\n",
      " [ 13  35 284]]\n",
      "\n",
      "================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  test_damagenet_text_final.py\n",
    "#  Standalone testing script for DamageNetTextFromScratch\n",
    "# ============================================================\n",
    "\n",
    "import os, re, json, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG (EDIT THESE)\n",
    "# ------------------------------------------------------------\n",
    "VOCAB_PATH = \"/Volumes/Extreme SSD/DL_Proj/damagenet_text_minority_ckpt/vocab.json\"\n",
    "CHECKPOINT_PATH = \"/Volumes/Extreme SSD/DL_Proj/damagenet_text_minority_ckpt/best_minority.pt\"\n",
    "\n",
    "TEST_FILE = \"/Volumes/Extreme SSD/DL_Proj/CrisisMMD_v2.0/crisismmd_datasplit_all/task_damage_text_img_test.tsv\"\n",
    "\n",
    "MAX_LEN = 128\n",
    "EMB_DIM = 256\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 4\n",
    "FF_DIM = 512\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UTILITIES\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s.,!?']\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    return re.findall(r\"\\b[\\w']+\\b\", text.lower())\n",
    "\n",
    "def encode_text_ids_from_tokens(tokens, vocab, max_len=MAX_LEN):\n",
    "    ids = [vocab.get(t, vocab[\"<unk>\"]) for t in tokens[:max_len - 1]]\n",
    "    ids = [vocab[\"<cls>\"]] + ids\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOAD VOCAB\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Loading vocab:\", VOCAB_PATH)\n",
    "with open(VOCAB_PATH, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOAD TEST TSV\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Loading test file:\", TEST_FILE)\n",
    "df = pd.read_csv(TEST_FILE, sep=\"\\t\")\n",
    "df[\"tweet_text\"] = df[\"tweet_text\"].astype(str).apply(clean_text)\n",
    "\n",
    "label_le = LabelEncoder()\n",
    "df[\"label_id\"] = label_le.fit_transform(df[\"label\"].astype(str))\n",
    "n_classes = len(label_le.classes_)\n",
    "print(\"CLASSES:\", label_le.classes_.tolist())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DATASET\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        toks = basic_tokenizer(row[\"tweet_text\"])\n",
    "        ids = encode_text_ids_from_tokens(toks, self.vocab)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"label\": torch.tensor(int(row[\"label_id\"]), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "test_dataset = TestDataset(df, vocab)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MODEL ARCHITECTURE (MATCHES TRAINING)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True, dropout=dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = self.attn(x, x, x, key_padding_mask=mask)[0]\n",
    "        x = self.norm1(x + out)\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, EMB_DIM, padding_idx=0)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, EMB_DIM))\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(EMB_DIM, N_HEADS, FF_DIM, DROPOUT)\n",
    "            for _ in range(N_LAYERS)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(EMB_DIM)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        mask = ids.eq(0)\n",
    "        x = self.emb(ids) + self.pos[:, :ids.size(1)]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x[:, 0])\n",
    "\n",
    "class DamageNetTextFromScratch(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = TextEncoder(vocab_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(EMB_DIM, EMB_DIM // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(EMB_DIM // 2, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, ids, logit_adjust=None):\n",
    "        z = self.encoder(ids)\n",
    "        logits = self.classifier(z)\n",
    "        return logits + logit_adjust if logit_adjust is not None else logits\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOAD CHECKPOINT (HANDLES ALL FORMATS)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Loading checkpoint:\", CHECKPOINT_PATH)\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "\n",
    "model = DamageNetTextFromScratch(vocab_size, n_classes).to(DEVICE)\n",
    "\n",
    "if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "    print(\"Checkpoint format: {model_state: ...}\")\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    logit_adjust = ckpt.get(\"logit_adjust\", None)\n",
    "\n",
    "elif isinstance(ckpt, dict) and any(k.startswith(\"encoder\") or k.startswith(\"classifier\") for k in ckpt.keys()):\n",
    "    print(\"Checkpoint format: raw state_dict dict\")\n",
    "    model.load_state_dict(ckpt)\n",
    "    logit_adjust = None\n",
    "\n",
    "else:\n",
    "    print(\"Checkpoint format: raw tensor-only state_dict\")\n",
    "    model.load_state_dict(ckpt)\n",
    "    logit_adjust = None\n",
    "\n",
    "if logit_adjust is not None:\n",
    "    logit_adjust = torch.tensor(logit_adjust, dtype=torch.float32, device=DEVICE)\n",
    "    print(\"Loaded logit adjustment.\")\n",
    "else:\n",
    "    print(\"No logit adjustment found.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# EVALUATION LOOP\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nRunning inference...\")\n",
    "model.eval()\n",
    "preds = []\n",
    "trues = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        y = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        logits = model(ids, logit_adjust)\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        preds.extend(pred.cpu().tolist())\n",
    "        trues.extend(y.cpu().tolist())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# METRICS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "acc = accuracy_score(trues, preds)\n",
    "\n",
    "print(\"\\n================= TEST RESULTS =================\")\n",
    "print(f\"Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(classification_report(\n",
    "    trues,\n",
    "    preds,\n",
    "    target_names=label_le.classes_,\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(trues, preds))\n",
    "\n",
    "print(\"\\n================================================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
