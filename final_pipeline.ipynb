{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yatharthnehva/Desktop/DL/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All four models loaded\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    AutoModel,\n",
    "    CLIPModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModel as HF_AutoModel,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 256\n",
    "MAX_LEN  = 96\n",
    "\n",
    "T1_PATH = r\"/Users/yatharthnehva/Desktop/ALL_MODELS/T1_FUSION_FINAL.pt\"\n",
    "T2_PATH = r\"/Users/yatharthnehva/Desktop/ALL_MODELS/T2T_TEXT.pt\"\n",
    "T3_PATH = r\"/Users/yatharthnehva/Desktop/ALL_MODELS/T3_MULTIMODAL.pt\"\n",
    "T4_PATH = r\"/Users/yatharthnehva/Desktop/ALL_MODELS/T4_MULTIMODAL.pt\"\n",
    "\n",
    "class T1_FUSION(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.txt = HF_AutoModel.from_pretrained(\"distilroberta-base\")\n",
    "        self.img = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        for p in self.img.vision_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.txtp = nn.Linear(768, 256)\n",
    "        self.imgp = nn.Linear(768, 256)\n",
    "        self.fc   = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, ids, mask, img):\n",
    "        t = self.txt(input_ids=ids, attention_mask=mask).last_hidden_state[:, 0]\n",
    "        with torch.no_grad():\n",
    "            v = self.img.vision_model(img).pooler_output\n",
    "        fused = torch.cat([self.txtp(t), self.imgp(v)], dim=1)\n",
    "        return self.fc(fused)\n",
    "\n",
    "t1_tok = RobertaTokenizerFast.from_pretrained(\"distilroberta-base\")\n",
    "t1_img_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.481, 0.457, 0.408], [0.269, 0.261, 0.276]),\n",
    "])\n",
    "t1_id2label = {0: \"non_informative\", 1: \"informative\"}\n",
    "\n",
    "class T2TextModel(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.txt = HF_AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        H = self.txt.config.hidden_size\n",
    "        self.ln = nn.LayerNorm(H)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(H, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.txt(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        cls = out[:, 0, :]\n",
    "        cls = self.ln(cls)\n",
    "        return self.head(cls)\n",
    "\n",
    "t2_tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "t2_id2label = {0: \"humanitarian\", 1: \"non_informative\", 2: \"structure\"}\n",
    "\n",
    "class T3Multimodal(nn.Module):\n",
    "    def __init__(self, num_classes=3, backbone_name=\"convnext_tiny\"):\n",
    "        super().__init__()\n",
    "        self.img = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            drop_path_rate=0.0,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "            d_img = self.img(dummy).shape[-1]\n",
    "\n",
    "        self.txt = HF_AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        d_txt = self.txt.config.hidden_size\n",
    "        self.txt_ln = nn.LayerNorm(d_txt)\n",
    "\n",
    "        fused_dim = d_img + d_txt\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        f_img = self.img(image)\n",
    "        out_txt = self.txt(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        cls_txt = self.txt_ln(out_txt[:, 0, :])\n",
    "        fused = torch.cat([f_img, cls_txt], dim=-1)\n",
    "        return self.head(fused)\n",
    "\n",
    "t3_tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "t3_img_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
    "])\n",
    "t3_id2label = {\n",
    "    0: \"little_or_no_damage\",\n",
    "    1: \"mild_damage\",\n",
    "    2: \"severe_damage\",\n",
    "}\n",
    "\n",
    "class T4Multimodal(nn.Module):\n",
    "    def __init__(self, num_classes=3, backbone_name=\"convnext_tiny\"):\n",
    "        super().__init__()\n",
    "        self.img = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            drop_path_rate=0.1,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "            d_img = self.img(dummy).shape[-1]\n",
    "\n",
    "        self.txt = HF_AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        d_txt = self.txt.config.hidden_size\n",
    "        self.txt_ln = nn.LayerNorm(d_txt)\n",
    "\n",
    "        fused_dim = d_img + d_txt\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        f_img = self.img(image)\n",
    "        out_txt = self.txt(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        cls_txt = self.txt_ln(out_txt[:, 0, :])\n",
    "        fused = torch.cat([f_img, cls_txt], dim=-1)\n",
    "        return self.head(fused)\n",
    "\n",
    "t4_tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "t4_img_tfms = t3_img_tfms\n",
    "t4_id2label = {\n",
    "    0: \"people_affected\",\n",
    "    1: \"rescue_needed\",\n",
    "    2: \"no_human\",\n",
    "}\n",
    "\n",
    "t1_model = T1_FUSION().to(DEVICE)\n",
    "t1_model.load_state_dict(torch.load(T1_PATH, map_location=DEVICE))\n",
    "t1_model.eval()\n",
    "\n",
    "t2_model = T2TextModel(num_classes=3).to(DEVICE)\n",
    "t2_model.load_state_dict(torch.load(T2_PATH, map_location=DEVICE))\n",
    "t2_model.eval()\n",
    "\n",
    "t3_model = T3Multimodal(num_classes=3).to(DEVICE)\n",
    "t3_model.load_state_dict(torch.load(T3_PATH, map_location=DEVICE))\n",
    "t3_model.eval()\n",
    "\n",
    "t4_model = T4Multimodal(num_classes=3).to(DEVICE)\n",
    "t4_model.load_state_dict(torch.load(T4_PATH, map_location=DEVICE))\n",
    "t4_model.eval()\n",
    "\n",
    "print(\"✅ All four models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All four models loaded\n",
      "{'task1': 'non_informative', 'task2': 'structure', 'task3': 'severe_damage', 'task4': 'people_affected'}\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    AutoModel,\n",
    "    CLIPModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModel as HF_AutoModel,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 256\n",
    "MAX_LEN  = 96\n",
    "\n",
    "T1_PATH = r\"/Users/yatharthnehva/Desktop/ALL_MODELS/T1_FUSION_FINAL.pt\"\n",
    "T2_PATH = r\"/Users/yatharthnehva/Desktop/ALL_MODELS/T2T_TEXT.pt\"\n",
    "T3_PATH = r\"/Users/yatharthnehva/Desktop/ALL_MODELS/T3_MULTIMODAL.pt\"\n",
    "T4_PATH = r\"/Users/yatharthnehva/Desktop/ALL_MODELS/T4_MULTIMODAL.pt\"\n",
    "\n",
    "class T1_FUSION(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.txt = HF_AutoModel.from_pretrained(\"distilroberta-base\")\n",
    "        self.img = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        for p in self.img.vision_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.txtp = nn.Linear(768, 256)\n",
    "        self.imgp = nn.Linear(768, 256)\n",
    "        self.fc   = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, ids, mask, img):\n",
    "        t = self.txt(input_ids=ids, attention_mask=mask).last_hidden_state[:, 0]\n",
    "        with torch.no_grad():\n",
    "            v = self.img.vision_model(img).pooler_output\n",
    "        fused = torch.cat([self.txtp(t), self.imgp(v)], dim=1)\n",
    "        return self.fc(fused)\n",
    "\n",
    "t1_tok = RobertaTokenizerFast.from_pretrained(\"distilroberta-base\")\n",
    "t1_img_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.481, 0.457, 0.408], [0.269, 0.261, 0.276]),\n",
    "])\n",
    "t1_id2label = {0: \"non_informative\", 1: \"informative\"}\n",
    "\n",
    "class T2TextModel(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.txt = HF_AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        H = self.txt.config.hidden_size\n",
    "        self.ln = nn.LayerNorm(H)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(H, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.txt(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        cls = out[:, 0, :]\n",
    "        cls = self.ln(cls)\n",
    "        return self.head(cls)\n",
    "\n",
    "t2_tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "t2_id2label = {0: \"humanitarian\", 1: \"non_informative\", 2: \"structure\"}\n",
    "\n",
    "class T3Multimodal(nn.Module):\n",
    "    def __init__(self, num_classes=3, backbone_name=\"convnext_tiny\"):\n",
    "        super().__init__()\n",
    "        self.img = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            drop_path_rate=0.0,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "            d_img = self.img(dummy).shape[-1]\n",
    "\n",
    "        self.txt = HF_AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        d_txt = self.txt.config.hidden_size\n",
    "        self.txt_ln = nn.LayerNorm(d_txt)\n",
    "\n",
    "        fused_dim = d_img + d_txt\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        f_img = self.img(image)\n",
    "        out_txt = self.txt(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        cls_txt = self.txt_ln(out_txt[:, 0, :])\n",
    "        fused = torch.cat([f_img, cls_txt], dim=-1)\n",
    "        return self.head(fused)\n",
    "\n",
    "t3_tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "t3_img_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
    "])\n",
    "t3_id2label = {\n",
    "    0: \"little_or_no_damage\",\n",
    "    1: \"mild_damage\",\n",
    "    2: \"severe_damage\",\n",
    "}\n",
    "\n",
    "class T4Multimodal(nn.Module):\n",
    "    def __init__(self, num_classes=3, backbone_name=\"convnext_tiny\"):\n",
    "        super().__init__()\n",
    "        self.img = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            drop_path_rate=0.1,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "            d_img = self.img(dummy).shape[-1]\n",
    "\n",
    "        self.txt = HF_AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        d_txt = self.txt.config.hidden_size\n",
    "        self.txt_ln = nn.LayerNorm(d_txt)\n",
    "\n",
    "        fused_dim = d_img + d_txt\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        f_img = self.img(image)\n",
    "        out_txt = self.txt(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        cls_txt = self.txt_ln(out_txt[:, 0, :])\n",
    "        fused = torch.cat([f_img, cls_txt], dim=-1)\n",
    "        return self.head(fused)\n",
    "\n",
    "t4_tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "t4_img_tfms = t3_img_tfms\n",
    "t4_id2label = {\n",
    "    0: \"people_affected\",\n",
    "    1: \"rescue_needed\",\n",
    "    2: \"no_human\",\n",
    "}\n",
    "\n",
    "t1_model = T1_FUSION().to(DEVICE)\n",
    "t1_model.load_state_dict(torch.load(T1_PATH, map_location=DEVICE))\n",
    "t1_model.eval()\n",
    "\n",
    "t2_model = T2TextModel(num_classes=3).to(DEVICE)\n",
    "t2_model.load_state_dict(torch.load(T2_PATH, map_location=DEVICE))\n",
    "t2_model.eval()\n",
    "\n",
    "t3_model = T3Multimodal(num_classes=3).to(DEVICE)\n",
    "t3_model.load_state_dict(torch.load(T3_PATH, map_location=DEVICE))\n",
    "t3_model.eval()\n",
    "\n",
    "t4_model = T4Multimodal(num_classes=3).to(DEVICE)\n",
    "t4_model.load_state_dict(torch.load(T4_PATH, map_location=DEVICE))\n",
    "t4_model.eval()\n",
    "\n",
    "print(\"✅ All four models loaded\")\n",
    "\n",
    "tweet_text = (\n",
    "    \"A large apartment building has collapsed after the earthquake, the entire street is underwater, \"\n",
    "    \"and dozens of injured people are still trapped inside waiting for rescue teams to arrive.\"\n",
    ")\n",
    "\n",
    "def load_image_for_t1(path=None):\n",
    "    img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "    return t1_img_tfms(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "def load_image_for_t34(path=None):\n",
    "    img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
    "    return t3_img_tfms(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "def encode_t1_text(text):\n",
    "    enc = t1_tok(text, padding=\"max_length\", truncation=True, max_length=72, return_tensors=\"pt\")\n",
    "    return enc[\"input_ids\"].to(DEVICE), enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "def encode_t2_text(text):\n",
    "    enc = t2_tok(text, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return enc[\"input_ids\"].to(DEVICE), enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "def encode_t3_text(text):\n",
    "    enc = t3_tok(text, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return enc[\"input_ids\"].to(DEVICE), enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "def encode_t4_text(text):\n",
    "    enc = t4_tok(text, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return enc[\"input_ids\"].to(DEVICE), enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_full_pipeline(text):\n",
    "    # Task 1\n",
    "    ids1, mask1 = encode_t1_text(text)\n",
    "    img1 = load_image_for_t1()\n",
    "    logits1 = t1_model(ids1, mask1, img1)\n",
    "    t1_pred = logits1.argmax(1).item()\n",
    "    t1_label = t1_id2label[t1_pred]\n",
    "\n",
    "    # Task 2\n",
    "    ids2, mask2 = encode_t2_text(text)\n",
    "    logits2 = t2_model(ids2, mask2)\n",
    "    t2_pred = logits2.argmax(1).item()\n",
    "    t2_label = t2_id2label[t2_pred]\n",
    "\n",
    "    img34 = load_image_for_t34()\n",
    "\n",
    "    # Task 3 (always)\n",
    "    ids3, mask3 = encode_t3_text(text)\n",
    "    logits3 = t3_model(ids3, mask3, img34)\n",
    "    t3_pred = logits3.argmax(1).item()\n",
    "    t3_label = t3_id2label[t3_pred]\n",
    "\n",
    "    # Task 4 (always)\n",
    "    ids4, mask4 = encode_t4_text(text)\n",
    "    logits4 = t4_model(ids4, mask4, img34)\n",
    "    t4_pred = logits4.argmax(1).item()\n",
    "    t4_label = t4_id2label[t4_pred]\n",
    "\n",
    "    return {\n",
    "        \"task1\": t1_label,\n",
    "        \"task2\": t2_label,\n",
    "        \"task3\": t3_label,\n",
    "        \"task4\": t4_label,\n",
    "    }\n",
    "\n",
    "result = run_full_pipeline(tweet_text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
