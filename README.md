---

# üö® DisasterAI ‚Äì Multimodal Crisis Understanding

DisasterAI is a fusion pipeline for **marking safe zones and understanding on-ground situations** during crises using:

* social media posts (tweets + images)
* satellite / aerial imagery (pre‚Äì and post-disaster)

The repository contains **four classification models (T1‚ÄìT4)** plus a **Siamese damage model (SEG)** and a **final integrated pipeline** that ties them together.

---

## üî• Overview

The long-term goal is to move toward a **disaster foundation model** that can:

* jointly learn from **text, RGB photos, and satellite imagery**
* generalize across **events, locations, and label schemes**
* support **flexible decision-making**: identifying safe zones, prioritizing rescue, and understanding damage patterns.

This repo is a step in that direction: a **5-model system** trained on CrisisMMD and xView2-style data, with a unified pipeline for running them together.

---

## üß† Model Suite

> All text encoders use **DistilBERT**, and all image encoders use **ConvNeXt-Tiny** unless otherwise noted.

| Model                                | Input                     | Output                          | Purpose                                     |
| ------------------------------------ | ------------------------- | ------------------------------- | ------------------------------------------- |
| **T1 ‚Äì Fusion Relevance Classifier** | Tweet text + tweet image  | Informative / Non-Informative   | Filters crisis data that is actually useful |
| **T2 ‚Äì Text-Only Humanitarian**      | Tweet text                | Humanitarian / Structure / None | High-recall humanitarian signal extraction  |
| **T3 ‚Äì Light Multimodal Damage**     | Tweet text + image        | Little / Mild / Severe damage   | Damage severity estimation around events    |
| **T4 ‚Äì Multimodal Subtypes**         | Tweet text + image        | People_Affected / Rescue / None | Distinguishes people vs rescue vs no-human  |
| **SEG ‚Äì Siamese Damage Classifier**  | Pre + Post satellite crop | No / Minor / Major / Destroyed  | Structure-level change / damage reasoning   |

---

## üß± Architecture Details

### **T1 ‚Äì Fusion Relevance**

* **Inputs:** tweet text + associated image
* **Text branch:** DistilBERT ‚Üí CLS vector (768-dim) ‚Üí LayerNorm
* **Image branch:** ConvNeXt-Tiny ‚Üí pooled feature (e.g., 768-dim)
* **Fusion:** concatenation `[img, text] ‚Üí Linear ‚Üí GELU ‚Üí Linear ‚Üí logits(2)`
* **Training objective:** weighted cross-entropy with class weights from CrisisMMD label distribution
* **Use:** screen out non-informative posts early in the pipeline.

---

### **T2 ‚Äì Text-Only Humanitarian**

* **Inputs:** tweet text only
* **Encoder:** DistilBERT (base uncased)
* **Head:**

  * CLS (768) ‚Üí LayerNorm ‚Üí Linear(768‚Üí256) ‚Üí GELU ‚Üí Dropout ‚Üí Linear(256‚Üí3)
* **Labels (compressed from 8-way):**

  * `humanitarian` ‚Äì affected people, injuries, missing, rescue/donation
  * `structure` ‚Äì infrastructure or vehicle damage
  * `non_informative` ‚Äì other/irrelevant
* **Role:** provides a **language-only** view of humanitarian intent and type.

---

### **T3 ‚Äì Light Multimodal Damage**

* **Inputs:** tweet text + associated image
* **Image encoder:** ConvNeXt-Tiny (no classification head, pooled features)
* **Text encoder:** DistilBERT (CLS token)
* **Fusion head:**

  * `[img, text]` concat ‚Üí Linear(fused_dim‚Üí256) ‚Üí GELU ‚Üí Dropout
  * Linear(256‚Üí64) ‚Üí GELU ‚Üí Linear(64‚Üí3)
* **Labels:** `little_or_no_damage`, `mild_damage`, `severe_damage`
* **Observation:** strong performance on **severe damage**, confusion between **little vs mild** due to visual ambiguity and class imbalance.

---

### **T4 ‚Äì Multimodal Humanitarian Subtypes**

* **Inputs:** same tweet text + image pair
* **Encoders:** ConvNeXt-Tiny (image) + DistilBERT (text), identical to T3
* **Fusion:** same late-fusion MLP as T3, but trained for different labels
* **Labels:**

  * `people_affected` ‚Äì affected, injured, or missing individuals
  * `rescue_needed` ‚Äì volunteering, donations, search & rescue
  * `no_human` ‚Äì damage / context information without explicit people
* **Performance:**

  * Strong overall F1 (~0.85), best among the suite
  * Slightly weaker recall for people_affected due to class imbalance.

---

### **SEG ‚Äì Siamese Damage Segmentation / Classifier**

Implemented in `segmentation.ipynb` with a MiT-B1 backbone.

* **Inputs:** crop around a structure from **pre-disaster** and **post-disaster** satellite imagery
* **Encoder:** `SiameseMiTEncoder`

  * shared MiT-B1 encoder (from `segmentation_models_pytorch`)
  * each image ‚Üí feature map `(B, 512, 7, 7)` ‚Üí global average pooling ‚Üí `(B, 512)`
* **Fusion + classifier (`DamageClassifier`):**

  * Concatenate `[f_pre, f_post]` ‚Üí `(B, 1024)`
  * Fusion MLP: `1024 ‚Üí 512 ‚Üí 512` with ReLU
  * Classifier: `512 ‚Üí 128 ‚Üí 4`
* **Outputs:** logits for `["no-damage", "minor-damage", "major-damage", "destroyed"]`
* **Use:** building-level damage reasoning based on **change**, not just appearance.

---

## üåê End-to-End Pipeline

The scripts `final_pipeline.ipynb` and `final_segmentandtweet_pipeline.py` connect all pieces:

1. **Satellite Damage Analysis (SEG)**

   * Take pre‚Äì and post-disaster imagery.
   * Run the Siamese MiT damage model to classify building damage.
   * Optionally overlay results as a damage map (from `segmentation.ipynb`).

2. **Social Media Filtering (T1)**

   * For each tweet + image, run T1 to keep only **informative** posts.

3. **Humanitarian Interpretation (T2 + T4)**

   * T2 (text-only) provides robust humanitarian vs structure vs non-info view.
   * T4 (multimodal) sharpens focus on **people_affected** and **rescue_needed**.

4. **Damage Context (T3)**

   * T3 links the social media image/text to coarse damage severity around the event.

5. **Safe Zone / Situation View**

   * Combine:

     * SEG‚Äôs **spatial damage map**, and
     * T1‚ÄìT4 outputs (what people say + what images show)
   * to identify regions with:

     * high people_affected + high damage ‚Üí **priority rescue**
     * low damage + no_human ‚Üí potential **safe zones / staging areas**.

---

## üìÅ Repository Structure

```bash
/
‚îú‚îÄ‚îÄ README.md                           # Project documentation
‚îÇ
‚îú‚îÄ‚îÄ T1.py                               # T1 ‚Äì Informative vs Non-Informative (fusion)
‚îú‚îÄ‚îÄ T2.py                               # T2 ‚Äì Human / Structure / Non-Informative (text-only)
‚îú‚îÄ‚îÄ T3.py                               # T3 ‚Äì Multimodal damage classifier
‚îú‚îÄ‚îÄ T4.py                               # T4 ‚Äì Multimodal humanitarian subtype classifier
‚îÇ
‚îú‚îÄ‚îÄ architecture_redefinition.py        # Shared multi-modal architecture definitions
‚îú‚îÄ‚îÄ final_pipeline.ipynb                # Jupyter notebook: multi-model pipeline experiments
‚îú‚îÄ‚îÄ final_segmentandtweet_pipeline.py   # Final Python pipeline: segmentation + tweets
‚îÇ
‚îú‚îÄ‚îÄ kaggle_textmodel_main.py            # Kaggle execution entrypoint for text models
‚îú‚îÄ‚îÄ main_DL.ipynb                       # Early development / prototyping notebook
‚îú‚îÄ‚îÄ main_DL_text.py                     # Standalone text model experimentation
‚îÇ
‚îú‚îÄ‚îÄ retraining_text_model.py            # Resume / fine-tune DistilBERT text model
‚îÇ
‚îî‚îÄ‚îÄ segmentation.ipynb                  # SEG model + damage overlay visualizations
```

---

## ‚öôÔ∏è Installation

```bash
git clone https://github.com/Ynehra24/DLPROJ.git
cd DLPROJ
pip install -r requirements.txt
```

(If you use different envs for Kaggle / local, list them in the repo.)

---

## üìä Datasets

* **CrisisMMD** ‚Äì multimodal social media crisis dataset
  [https://crisisnlp.qcri.org/crisismmd](https://crisisnlp.qcri.org/crisismmd)
* **xView2 (or similar)** ‚Äì pre/post satellite building damage dataset
  [https://xview2.org/](https://xview2.org/)

---

## üöÄ Training / Re-Training

> These commands assume dataset paths are configured inside each script or via argparse flags.

```bash
# Tweet relevance and humanitarian tasks
python T1.py         # Train/finetune T1 fusion model
python T2.py         # Train/finetune T2 text-only humanitarian model
python T3.py         # Train/finetune T3 multimodal damage model
python T4.py         # Train/finetune T4 multimodal subtype model

# SEG Siamese damage model (usually run as notebook, but logic can be scripted)
# segmentation.ipynb ‚Äì training and visualization
```

For **resume training / Kaggle checkpoints**:

```bash
python retraining_text_model.py
python kaggle_textmodel_main.py
```

---

## üîç Inference / Testing

Example CLI patterns (adapt to your actual arguments):

```bash
# T1: relevance
python T1.py --mode test --img example.jpg --text "Sample tweet text"

# T2: humanitarian / structure / non-info
python T2.py --mode test --tsv data/task2_test.tsv

# T3: damage severity from tweet + image
python T3.py --mode test --img example.jpg --text "..."

# T4: subtype classification
python T4.py --mode test --img example.jpg --text "..."

# Final combined pipeline: segmentation + tweet models
python final_segmentandtweet_pipeline.py \
    --pre pre_disaster.png \
    --post post_disaster.png \
    --tweets tweets.json
```

(If your scripts use different flags, you can adjust these examples.)

---

## üìä Performance Summary

| Model   | Metric (Test)            | Notes                                        |
| ------- | ------------------------ | -------------------------------------------- |
| **T1**  | F1 = **0.8318**          | Fusion outperforms text-only & image-only    |
| **T2**  | F1 = **0.8297**          | Strong humanitarian detection; text-focused  |
| **T3**  | F1 = **0.6106**          | High for severe damage; mild vs little noisy |
| **T4**  | F1 = **0.8502**          | Best overall; strong on no_human + rescue    |
| **SEG** | Weighted F1 ‚âà **0.8854** | Very effective at pre/post change detection  |

(You can expand this with per-class F1 tables you already computed.)

---

## üìå Key Learnings

* **Multimodal > Single-Modality**
  Fusion of text + image consistently beats isolated branches, especially for relevance (T1) and subtypes (T4).

* **Imbalance Hurts the Rare Classes**
  Mild damage, structure, and people_affected are under-represented and show reduced F1; balancing or reweighting is critical.

* **Images Encode Damage, Text Encodes Intent**
  Vision models shine on structural damage, while text models excel at describing **who** is affected and **what** is happening.

* **Siamese Pre/Post Modelling is Powerful**
  SEG‚Äôs MiT-B1 Siamese encoder learns **change** rather than static appearance and is robust for structure-level damage analysis.

* **Towards a Foundation-Style Disaster Model**
  Combining SEG + T1‚ÄìT4 into a single pipeline provides a blueprint for a future **multi-task, multi-modal disaster foundation model**.

---

## üìÑ License
`Apache 2.0`

---

## ü§ù Credits

* Authors and contributors of this repository
* CrisisMMD & xView2 dataset creators
* DistilBERT, ConvNeXt, MiT, and segmentation_models_pytorch authors
* All upstream open-source libraries used in this project

---
